{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfLWRGflXZvxujkKNiSzZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CNN for image classifyng (binomial CNN)"],"metadata":{"id":"b0fBC8Bt3XUz"}},{"cell_type":"markdown","source":["## 1 Setup and loading data"],"metadata":{"id":"xdAPR9mV4N2n"}},{"cell_type":"markdown","source":["### 1.1 Install dependencies and setup"],"metadata":{"id":"nA8NCWU43j11"}},{"cell_type":"code","source":["pip install tensorflow tensorflow-gpu opencv-python matplotlib"],"metadata":{"id":"hod6hyf6cPby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","import cv2\n","import imghdr\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","# Deep Learning Model\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n","\n","# Model Evaluation\n","\n","from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n","\n","# Saving the model\n","\n","from tensorflow.keras.models import load_model"],"metadata":{"id":"1vWJFxmMcKuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 Load Data"],"metadata":{"id":"QM-gyEvK4KaE"}},{"cell_type":"code","source":["#Known data directory, this directory has to have all data sorted. Class 1 \n","# images on a directory, class 2 images in another directory and so on.\n","data_dir = 'path_to_data_directory'"],"metadata":{"id":"2FNnX1fC4cQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code line to know which classes 'data_dir' has inside\n","os.listdir(data_dir)"],"metadata":{"id":"FzsDcIRf4_JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code line to automatically indicate the code the data directory and how big \n","# the batches of images will be\n","# tf.keras.utils.image_dataset_from_directory?? can help to check other dataset\n","# parameters\n","\n","data = tf.keras.utils.image_dataset_from_directory('path_to_data_directory', \n","                                                   batch_size = 15)"],"metadata":{"id":"4xVW7jty5Un2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual representation of the data set\n","data_iterator = data.as_numpy_iterator()\n","batch = data_iterator.next()"],"metadata":{"id":"5PtcFrky7fBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(ncols=8, figsize = (20,20))\n","for idx, img in enumerate(batch[0][:8]):\n","  ax[idx].imshow(img.astype(int))\n","  ax[idx].title.set_text(batch[1][idx])"],"metadata":{"id":"Abzxdtvp7zlV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Shows the number of images in the batch, dimension in pixels and channels (RGB)\n","batch[0].shape"],"metadata":{"id":"bRE_l-rf72FP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Shows the tags of the displayed images\n","# Classes are representen by numbers starting by 0\n","# Note: The order of the classes is asigned in alphabetic order \n","batch[1]"],"metadata":{"id":"N0cn74HS8M0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2 Image Preprocessing"],"metadata":{"id":"1mFG0AZ88uLC"}},{"cell_type":"markdown","source":["### 2.1 Scale Images"],"metadata":{"id":"r3KQi9IL87Qi"}},{"cell_type":"code","source":["# Scaling the image means that the pixels that had a numerical value between\n","# 0 and 255, now will have a value between 0 and 1 in order to facilitate the\n","# calculus. \n","\n","data = data.map(lambda x,y: (x/255, y))"],"metadata":{"id":"pVyrL1Io9EFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Proof tha the max value is 1\n","batch[0].max()"],"metadata":{"id":"LRJQi5j99oid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Proof tha the min value is 0\n","batch[0].min()"],"metadata":{"id":"Feb5BJ8d92TY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual representation of the scaled data\n","scaled_iterator = data.as_numpy_iterator()\n","batch = scaled_iterator.next()"],"metadata":{"id":"HCemP68H97vY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(ncols=8, figsize = (20,20))\n","for idx, img in enumerate(batch[0][:8]):\n","  ax[idx].imshow(img)\n","  ax[idx].title.set_text(batch[1][idx])"],"metadata":{"id":"foX8aUdU-EGg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Split Data"],"metadata":{"id":"Lptgecth-SIr"}},{"cell_type":"code","source":["# This will let us know how many batches we have\n","# A batch can be seen as an array that contains info of given quantity of images\n","# (this quantity was given in the 3rd code line of '1.2 Load Data')\n","len(data)"],"metadata":{"id":"6tmQ0oVc-a-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the # of batches that will be used for training (~70% of data)\n","train_size = int(len(data)*.7)\n","train_size"],"metadata":{"id":"BKHQAPp-_Ifj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the # of batches that will be used for validation (~20% of data)\n","val_size = int(len(data)*.2)+1\n","val_size"],"metadata":{"id":"xx0mcs0-_gJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the # of batches that will be used for testing (~10% of data)\n","test_size = int(len(data)*.1)+1\n","test_size"],"metadata":{"id":"oORD4qmB_mMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sorting data \n","train = data.take(train_size)\n","val = data.skip(train_size).take(val_size)\n","test = data.skip(train_size + val_size).take(test_size)"],"metadata":{"id":"WVFEbipS_w3r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3 Deep Learning Model"],"metadata":{"id":"CTfa9qDkAEk2"}},{"cell_type":"markdown","source":["### 3.1 Construction of the deep learning model"],"metadata":{"id":"8ioXiz2vJkr4"}},{"cell_type":"code","source":["# Define the type of model\n","# Sequential means that there is just one kind of input and only one output\n","model = Sequential()"],"metadata":{"id":"H_OKi0PrI6ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tf.keras.layers?? Manual to the different kind of layer"],"metadata":{"id":"cSkt2vCIJSwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Architecture design\n","\n","model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape = (256,256,3)))\n","model.add(MaxPooling2D())\n","# 16 is the number of filters\n","# (3,3) size of the filter\n","# 1 means that the convolution will go trhoug every single part of the image\n","\n","model.add(Conv2D(32, (3,3), 1, activation = 'relu'))\n","model.add(MaxPooling2D())\n","\n","model.add(Conv2D(16, (3,3), 1, activation = 'relu'))\n","model.add(MaxPooling2D())\n","\n","model.add(Flatten())\n","\n","model.add(Dense(256, activation = 'relu'))\n","model.add(Dense(1, activation = 'sigmoid')) \n","# I used sigmoid because it is a binomial CNN so the sigmoid activation function\n","# works well for binary classification because it just has two outputs, 0 or 1\n"],"metadata":{"id":"kvw_xwASJgfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This code line allows to compile the CNN but it is intended to work for binary\n","# classification\n","model.compile('adam', loss = tf.losses.BinaryCrossentropy(), metrics = ['accuracy'])"],"metadata":{"id":"gvUnyydnKUzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"y3M2cuT5KkUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Training the CNN"],"metadata":{"id":"rAz7pRlZK3LQ"}},{"cell_type":"code","source":["# Establish directory for callbacks \n","logdir = 'path_to_logs_directory' \n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"],"metadata":{"id":"0o5sP7TfLK5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model\n","# Set # of epochs\n","hist = model.fit(train, epochs = 100, validation_data = val, callbacks = [tensorboard_callback])"],"metadata":{"id":"w1bO7kcXLcLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Training and validation Performance"],"metadata":{"id":"HbvkZWVgL36i"}},{"cell_type":"code","source":["#Visual representation of the training and validation loss performance\n","fig = plt.figure()\n","plt.plot(hist.history['loss'], color = 'teal', label = 'loss in training')\n","plt.plot(hist.history['val_loss'], color = 'orange', label = 'loss in validation')\n","fig.suptitle('loss', fontsize = 20)\n","plt.legend(loc = \"upper left\")\n","plt.show()"],"metadata":{"id":"4CZRNmGkLrMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visual representation of the training and validation accuracy performance\n","fig = plt.figure()\n","plt.plot(hist.history['accuracy'], color = 'teal', label = 'Accuracy in training')\n","plt.plot(hist.history['val_accuracy'], color = 'orange', label = 'Accuracy in validation')\n","fig.suptitle('Accuracy', fontsize = 20)\n","plt.legend(loc = \"lower right\")\n","plt.show()"],"metadata":{"id":"Q5yGvl8CMNro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Testing the model"],"metadata":{"id":"5ANMjTmQMiB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the metrics related to confusion matrix\n","pre = Precision()\n","rec = Recall()\n","acc = BinaryAccuracy()"],"metadata":{"id":"pFRkmZUdMn79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the test data set to perform testing\n","for batch in test.as_numpy_iterator():\n","  x, y = batch\n","  yhat = model.predict(x)\n","  pre.update_state(y, yhat)\n","  rec.update_state(y, yhat)\n","  acc.update_state(y, yhat)"],"metadata":{"id":"yK0Ox7J9MwIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the values obtained for the metrics\n","print(f'Precision:{pre.result().numpy()}, Recall:{rec.result().numpy()}, Accuracy:{acc.result().numpy()}')"],"metadata":{"id":"tKGoMAEeNALh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 Final test"],"metadata":{"id":"-kYDbf7mNOsa"}},{"cell_type":"code","source":["# I suggest that for this part, we can use images that haven't been part of any\n","# of the datasets (training, validation and test) they could be new images.\n","\n","# Load the new image and visualize it\n","img = cv2.imread('/content/drive/MyDrive/CNN_tesis/RNC_sex_id_mnov_bb/test/hembra_test_mnov_00516.png')\n","plt.imshow(img)\n","plt.show()"],"metadata":{"id":"vH29EQI4NRmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Resize the new image\n","resize = tf.image.resize(img, (256,256))\n","plt.imshow(resize.numpy().astype(int))\n","plt.show()"],"metadata":{"id":"J0ZDKR4qNsYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scale and pass the image trhoug the CNN\n","yhat = model.predict(np.expand_dims(resize/255, 0))"],"metadata":{"id":"1jLNRi4LN0Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the final value (this value is the one requiered for the classification)\n","yhat"],"metadata":{"id":"7YZ-2vaQOEJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I used a sigmoidal function so all the values below 0.5 will become 0 (class 1) \n","# and all the values higher than 0.5 will become 1 (class 2)\n","if yhat > 0.5:\n","  print(f'It is predicted to be class 2')\n","else:\n","  print(f'It is predicted to be class 1')"],"metadata":{"id":"a5FooFWZOoZk"},"execution_count":null,"outputs":[]}]}